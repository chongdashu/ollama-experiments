{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a very basic notebook that showcases how to interface with a locally running Ollama instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "### Get Ollama\n",
    "1. Run the `install_ollama.sh` script in the terminal. This should install Ollama in the `bin` directory.\n",
    "\n",
    "2. Run the `on_start.sh` script, if it was not already run when you started this studio. This will launch the Ollama server and pre-load the Llama3 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Dependencies\n",
    "We only need langchain to interface with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.1.20 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Ollama module from langchain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# State which model we want\n",
    "model = \"llama3\"\n",
    "\n",
    "# Initialise the model\n",
    "llm = Ollama(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Langsmith can be very helpful in the testing process of your AI model. Here are some ways it can assist:\\n\\n1. **Data validation**: Langsmith's ability to identify and correct grammatical errors, as well as suggest alternative phrases or sentences, can help you validate your data and ensure that it is accurate and consistent.\\n2. **Model evaluation**: By analyzing the output of your AI model, Langsmith can provide insights into how well your model is performing and where it may be struggling. This can help you identify areas for improvement and refine your training data or model architecture accordingly.\\n3. **Error analysis**: If your model is making errors, Langsmith can help you understand what's going wrong by identifying patterns in the incorrect outputs. This can inform improvements to your model or the training process.\\n4. **Test case generation**: Langsmith can generate test cases based on your input data and expected output, which can be used to evaluate your AI model's performance.\\n5. **Evaluation metrics**: Langsmith provides evaluation metrics that can help you measure the performance of your AI model, such as precision, recall, F1-score, and accuracy.\\n\\nSome specific testing scenarios where Langsmith can be useful include:\\n\\n1. **Language translation**: If you're building a machine translation system, Langsmith can help test the translated text for grammar, syntax, and meaning.\\n2. **Named entity recognition (NER)**: By analyzing the output of your NER model, Langsmith can help identify errors in identifying entities like people, organizations, or locations.\\n3. **Text classification**: For text classification tasks like sentiment analysis or topic modeling, Langsmith can evaluate the accuracy of your model's predictions and provide suggestions for improvement.\\n4. **Chatbots and conversational AI**: If you're building a chatbot or conversational AI system, Langsmith can help test the coherence, fluency, and relevance of the generated text.\\n\\nBy integrating Langsmith into your testing process, you can ensure that your AI model is robust, accurate, and reliable in its outputs.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke with a prompt, response as a whole.\n",
    "prompt = \"how can langsmith help with testing?\"\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languagem (not Langsmith) is a powerful tool that can be used to assist in testing, especially when it comes to natural language processing (NLP) and machine learning models. Here are some ways Languagem can help:\n",
      "\n",
      "1. **Testing NLP Models**: Languagem provides a range of test cases for NLP models such as text classification, sentiment analysis, entity recognition, and more. These tests can be used to verify the accuracy and performance of your models.\n",
      "2. **Automated Testing**: Languagem allows you to automate testing by generating a large number of test cases using various natural language inputs. This helps to identify any errors or inconsistencies in your model's behavior.\n",
      "3. **Testing for Bias**: Languagem can help detect bias in NLP models by analyzing the performance on different subsets of data, such as demographics, tone, and sentiment.\n",
      "4. **Generating Test Data**: Languagem can generate test data that is similar to real-world text data, which helps to ensure that your model is robust and generalizable.\n",
      "5. **Testing for Edge Cases**: Languagem can help identify edge cases or unusual scenarios that may not be well-covered by traditional testing methods.\n",
      "\n",
      "Some specific examples of how Languagem can assist with testing include:\n",
      "\n",
      "* Testing a sentiment analysis model by generating test cases with varying levels of positivity, negativity, and neutrality.\n",
      "* Verifying the accuracy of an entity recognition model by testing it on a dataset of text containing named entities (e.g., people, organizations, locations).\n",
      "* Checking for bias in a language translation model by analyzing its performance on texts from different languages and cultures.\n",
      "\n",
      "Overall, Languagem can help ensure that your NLP models are accurate, reliable, and free from biases, which is crucial for building trust with users."
     ]
    }
   ],
   "source": [
    "# Invoke with a prompt, response streamed.\n",
    "chunks = []\n",
    "async for chunk in llm.astream(prompt):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk, end=\"\", flush=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
